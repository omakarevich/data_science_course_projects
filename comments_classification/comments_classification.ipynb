{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The store is launching a new service. Now users can edit and add product descriptions. Customers propose their edits and comment others' edits. The store needs a tool that will search for toxic comments and submit them for moderation.\n",
    "\n",
    "Train the model to classify comments as positive and negative. We have a dataset with markup on the toxicity of revisions.\n",
    "\n",
    "Build a model with a quality metric * F1 * of at least 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents <a id='contents'></a></font>\n",
    "\n",
    "\n",
    "1.[About data](#meatdata)\n",
    "\n",
    "- 1.1. [Library import and check data](#library_table)\n",
    "- 1.2. [Duplicates and nulls](#nulls_dupls)\n",
    "- 1.3. [Clear text](#lemmas)\n",
    "- 1.4. [Conclusion](#conclusion_1)\n",
    "\n",
    "2.[Train](#training)\n",
    "\n",
    "- 2.1. [Split data](#split)\n",
    "- 2.2. [Vectorize and transform feauters](#vect_transform_feauters)\n",
    "- 2.3. [Get params](#params)\n",
    "- 2.4. [Train](#train)\n",
    "- 2.5. [Test](#test)\n",
    "- 2.6. [Conclusion](#conclusion_2)\n",
    "\n",
    "3.[Final conclusion](#conclusion_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  About data<a id='meatdata'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library import and check data<a id='library_table'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Helga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#library import\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check data\n",
    "toxic_comments = pd.read_csv('toxic_comments.csv')\n",
    "display(toxic_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates and nulls<a id='nulls_dupls'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check duplicates\n",
    "toxic_comments.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check nulls\n",
    "toxic_comments.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear text<a id='lemmas'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text to string\n",
    "toxic_comments['text'] = toxic_comments['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>Explanation Why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>D aww He matches this background colour I m se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>Hey man I m really not trying to edit war It s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>More I can t make any real suggestions on impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>You sir are my hero Any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  Explanation Why the edits made under my userna...  \n",
       "1  D aww He matches this background colour I m se...  \n",
       "2  Hey man I m really not trying to edit war It s...  \n",
       "3  More I can t make any real suggestions on impr...  \n",
       "4  You sir are my hero Any chance you remember wh...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#clear test function\n",
    "def clear_text(text):\n",
    "    new_text = re.sub(r'[^a-zA-Z]',' ', text)\n",
    "    return \" \".join(new_text.split())\n",
    "\n",
    "toxic_comments['lemm_text']  = toxic_comments['text'].apply(clear_text)\n",
    "display(toxic_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str.lower and type string\n",
    "toxic_comments['lemm_text'] = toxic_comments['lemm_text'].str.lower()\n",
    "toxic_comments['lemm_text'] = toxic_comments['lemm_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits make under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww he match this background colour i m seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man i m really not try to edit war it s ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i can t make any real suggestion on impro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir be my hero any chance you remember wha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                           lemm_text  \n",
       "0  explanation why the edits make under my userna...  \n",
       "1  d aww he match this background colour i m seem...  \n",
       "2  hey man i m really not try to edit war it s ju...  \n",
       "3  more i can t make any real suggestion on impro...  \n",
       "4  you sir be my hero any chance you remember wha...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lemmatize function\n",
    "def lemm_lemm(sentence):\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "\n",
    "    return \" \".join(([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)]))\n",
    "    \n",
    "\n",
    "toxic_comments['lemm_text']  = toxic_comments['lemm_text'].apply(lemm_lemm)\n",
    "display(toxic_comments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 3 columns):\n",
      "text         159571 non-null object\n",
      "toxic        159571 non-null int64\n",
      "lemm_text    159571 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#check types\n",
    "toxic_comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion <a id='conclusion_1'></a>\n",
    "\n",
    "We have 159571 entries and 2 columns:\n",
    "- text - message text;\n",
    "- toxic - indicator of the toxicity of the comment (0 - no, 1 - yes);\n",
    "We later added a third column:\n",
    "- lemm_text - lemmatized column text.\n",
    "\n",
    "There are no missing values or duplicates.\n",
    "\n",
    "For lemmatization: only letters and spaces were left, punctuation marks and numbers were removed.\n",
    "\n",
    "In the second function: the lemmatization - change word into its original form.\n",
    "\n",
    "\n",
    "[To contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Train<a id='training'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data<a id='split'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95742,)\n",
      "(31914,)\n",
      "(31915,)\n"
     ]
    }
   ],
   "source": [
    "#features&target\n",
    "features = toxic_comments['lemm_text']\n",
    "target = toxic_comments['toxic']\n",
    "\n",
    "#divide for two\n",
    "features_train_valid, features_test, target_train_valid, target_test = train_test_split(\n",
    "    features, target, test_size=0.20, random_state=12345)\n",
    "\n",
    "#and three\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train_valid, target_train_valid, test_size=0.250, random_state=12345)\n",
    "\n",
    "print(features_train.shape)\n",
    "print(features_valid.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize and transform feauters<a id='vect_transform_feauters'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95742, 110880)\n",
      "(31914, 110880)\n",
      "(31915, 110880)\n"
     ]
    }
   ],
   "source": [
    "#TfidfVectorizer\n",
    "vect = TfidfVectorizer(stop_words=nltk_stopwords.words('english'),lowercase=True)\n",
    "\n",
    "\n",
    "new_features_train = vect.fit_transform(features_train)\n",
    "\n",
    "new_features_valid = vect.transform(features_valid)\n",
    "\n",
    "new_features_test = vect.transform(features_test)\n",
    "\n",
    "\n",
    "print(new_features_train.shape)\n",
    "print(new_features_valid.shape)\n",
    "print(new_features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get params<a id='params'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 15 : f1: 0.6446788111217642\n",
      "max_depth = 16 : f1: 0.6466882992937584\n",
      "max_depth = 17 : f1: 0.6520090978013646\n",
      "max_depth = 18 : f1: 0.6502463054187192\n",
      "max_depth = 19 : f1: 0.6600188146754468\n",
      "max_depth = 20 : f1: 0.6654177594604721\n"
     ]
    }
   ],
   "source": [
    "#params depth for DecisionTreeClassifier\n",
    "for depth in range(15, 21):\n",
    "    model =  DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model.fit(new_features_train, target_train)\n",
    "    predicted_valid = model.predict(new_features_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "    print(\"max_depth =\", depth, \": \", end='')\n",
    "    print(\"f1:\", f1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_est = 15 : f1: 0.69660014781966\n",
      "max_est = 16 : f1: 0.6789234268385141\n",
      "max_est = 17 : f1: 0.7014042867701404\n",
      "max_est = 18 : f1: 0.6863816161235637\n",
      "max_est = 19 : f1: 0.7023126734505087\n"
     ]
    }
   ],
   "source": [
    "#params n_estimators for RandomForestClassifier\n",
    "for est in range(15, 20):\n",
    "    model =  RandomForestClassifier(random_state=12345, n_estimators = est)\n",
    "    model.fit(new_features_train, target_train)\n",
    "    predicted_valid = model.predict(new_features_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "    print(\"max_est =\", est, \": \", end='')\n",
    "    print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf = 5 : f1: 0.7485051002462187\n",
      "min_data_in_leaf = 6 : f1: 0.750394667602175\n",
      "min_data_in_leaf = 7 : f1: 0.749472202674173\n",
      "min_data_in_leaf = 8 : f1: 0.7467509659290482\n",
      "min_data_in_leaf = 9 : f1: 0.748509294984216\n"
     ]
    }
   ],
   "source": [
    "#params for LightGBM min_data_in_leaf\n",
    "for min_data in range(5, 10):\n",
    "    model = LGBMClassifier(random_state=12345, min_data_in_leaf=min_data)\n",
    "    model.fit(new_features_train,target_train)\n",
    "    predicted_valid = model.predict(new_features_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "    print(\"min_data_in_leaf =\", min_data, \": \", end='') \n",
    "    print('f1:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 28 : f1: 0.7490333919156416\n",
      "max_depth = 29 : f1: 0.7499125568380552\n",
      "max_depth = 30 : f1: 0.750394667602175\n",
      "max_depth = 31 : f1: 0.750394667602175\n",
      "max_depth = 32 : f1: 0.750394667602175\n",
      "max_depth = 33 : f1: 0.750394667602175\n",
      "max_depth = 34 : f1: 0.750394667602175\n"
     ]
    }
   ],
   "source": [
    "#params for LightGBM max_depth\n",
    "for depth in range(28, 35):\n",
    "    model =  LGBMClassifier(random_state=12345,min_data_in_leaf=6, max_depth=depth)\n",
    "    model.fit(new_features_train,target_train)\n",
    "    predicted_valid = model.predict(new_features_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "    print(\"max_depth =\", depth, \": \", end='') \n",
    "    print('f1:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_leaves = 29 : f1: 0.7466007416563659\n",
      "num_leaves = 30 : f1: 0.7463946535349982\n",
      "num_leaves = 31 : f1: 0.750394667602175\n",
      "num_leaves = 32 : f1: 0.7491662278392137\n",
      "num_leaves = 33 : f1: 0.7519664394336656\n",
      "num_leaves = 34 : f1: 0.7526656178989686\n"
     ]
    }
   ],
   "source": [
    "#params for LightGBM num_leaves\n",
    "for nl in range(29, 35):\n",
    "    model =  LGBMClassifier(random_state=12345,min_data_in_leaf=6, max_depth=31, num_leaves=nl)\n",
    "    model.fit(new_features_train,target_train)\n",
    "    predicted_valid = model.predict(new_features_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "    print(\"num_leaves =\", nl, \": \", end='') \n",
    "    print('f1:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train<a id='train'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def classifier_model(models):\n",
    "    model = models\n",
    "    model.fit(new_features_train, target_train)\n",
    "    predicted_valid = model.predict(new_features_valid)\n",
    "    f1 = f1_score(target_valid, predicted_valid)\n",
    "    print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.7338444687842279\n"
     ]
    }
   ],
   "source": [
    "#train LogisticRegression\n",
    "classifier_model(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.7023126734505087\n"
     ]
    }
   ],
   "source": [
    "#train RandomForestClassifier\n",
    "classifier_model(RandomForestClassifier(random_state=12345, n_estimators = 19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.6654177594604721\n"
     ]
    }
   ],
   "source": [
    "#train DecisionTreeClassifier\n",
    "classifier_model(DecisionTreeClassifier(random_state=12345, max_depth= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.7470049330514447\n"
     ]
    }
   ],
   "source": [
    "#train LGBMClassifier\n",
    "classifier_model(LGBMClassifier(random_state=12345,max_depth=33, max_bin=6, min_data_in_leaf = 6,num_leaves=33))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Winner is  LGBMClassifier, check the test***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test<a id='test'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.7482993197278912\n"
     ]
    }
   ],
   "source": [
    "#test LGBMClassifier\n",
    "model = LGBMClassifier(random_state=12345,max_depth=33, max_bin=6, min_data_in_leaf = 6,num_leaves=33)\n",
    "model.fit(new_features_train, target_train)\n",
    "predicted_test = model.predict(new_features_test)\n",
    "f1 = f1_score(target_test, predicted_test)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion <a id='conclusion_2'></a>\n",
    "\n",
    "We have a standart data split, but something new in features.\n",
    "\n",
    "We take features and vectorize them. And also, for feauters_train, we configure the TF-IDF, for valid and target this is not necessary.\n",
    "\n",
    "So, for f1 the LGBMClassifier (0.75) won and LogisticRegression (0.73) was left behind.\n",
    "\n",
    "For the test, we take only the LGBMClassifier and the result is the desired f1 = 0.75.\n",
    "\n",
    "[To contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final conclusion  <a id='conclusion_total'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we had a nice data with 159571 entries and 2 columns: text and toxicity label (0/1).\n",
    "\n",
    "We've added another column with lemmatized and cleared text (in other words, we have returned the words to their original form). Then with the help of TfidfVectorizer and transform, we transformed the column for the model.\n",
    "\n",
    "We used:\n",
    "- Logistic regression: f1 = 0.73\n",
    "- Random forest: f1 = 0.70\n",
    "- Decision tree: f1 = 0.67\n",
    "- LGBMClassifier: f1 = 0.75\n",
    "\n",
    "As a result, winner is the LGBMClassifier and on test we see f1 = 0.75. \n",
    "\n",
    "\n",
    "[To contents](#contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
